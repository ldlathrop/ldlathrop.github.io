---
title: "Practical Machine Learning Project"
author: "L Lathrop"
date: "September 25, 2015"
output: html_document
---


### Introduction    
The use of devices such as *Jawbone Up, Nike FuelBand, and Fitbit* enables the collection of a large amount of data about personal activity. People who use such devices are able to quantify *how much* of a perticular activity they do, but they rarely quantify *how well* they do it. The goal of this project is to use data from the [Human Activity Recognition project](http://groupware.les.inf.puc-rio.br/har) to develop a model that predicts whether participants were performing barbell lifts correctly or incorrectly. The model will then be used on 20 different test cases.

### Loading global parameters 
```{r}
# Load the necessary packages
library(caret)
library(randomForest)
library(AppliedPredictiveModeling)
library(doMC)
library(RColorBrewer)
library(corrplot)
library(rattle)
```  
  
### Data processing and preprocessing  
We first read in the data from the training and testing sets.   
```{r cache=TRUE}
pmlTrain <- read.csv("pml-training.csv", header = TRUE, stringsAsFactors = FALSE)
pmlTest <- read.csv("pml-testing.csv", header = TRUE, stringsAsFactors = FALSE)

# Preprocess to remove columns with missing values
pmlTrain <- pmlTrain[, colSums(is.na(pmlTrain)) == 0]
pmlTest <- pmlTest[, colSums(is.na(pmlTest)) == 0]

# Preprocess to remove the unnecessary variables
pmlTrain <- pmlTrain[,-(1:7)]
pmlTest <- pmlTest[,-(1:7)]

# Transform 'classe' variable to factor variable
pmlTrain$classe <- as.factor(pmlTrain$classe)

# Now that we are done with preprocessing, we can segment the data set into training and validation sets
inTrain <- createDataPartition(y = pmlTrain$classe,
                               p = 0.60, list = FALSE)
training <- pmlTrain[inTrain,]
testing <- pmlTrain[-inTrain,]
dim(training)
```  
There are now 11,776 observations of 86 variables in the training set, which is still a great many variables. So we will perform further preprocessing of the data.
```{r}
# Remove non-zero values from training and test sets
nzv <- nearZeroVar(training, saveMetrics = TRUE)
nzv <- nearZeroVar(training)
filteredTrain <- training[, -nzv]
dim(filteredTrain)

nzv <- nearZeroVar(testing)
filteredTest <- testing[, -nzv]
```  
  
A correlation matrix (see Appendix A) shows that there are very few highly correlated variables, so it is not necessary to apply further preprocessing.  
  
### Data modeling  
The above gives us 11,776 observations of 53 variables in the training set. So we will fit a model using a Random Forest algorithm. Unlike bagging *(m = p)*, this method *(m < p)* automatically selects the most important variables and is more robust in terms of correlated covariates and outliers (James, Witten, Hastie, & Tibshirani, 2013). We will assess this using a *k*-fold cross validation because it gives more accurate estimates of the test error rate. In this case, *k* = 5 as this value has been shown empirically to yield test error rate estimates that have neither excessively high bias nor very high variance (James, et al., 2013).  
```{r cache = TRUE}
registerDoMC(cores = 3)
set.seed(1)
fitControl <- trainControl(method = "cv", number = 5, 
                           returnData = TRUE, 
                           savePredictions = TRUE,
                           classProbs = TRUE)
modelFit <- train(classe ~ ., data=filteredTrain, method= "rf",
                  trControl = fitControl, ntrees = 100,
                  allowParallel = TRUE, prox = TRUE)
modelFit

```  
  
The argument `mtry = 27` indicates that 27 of the 53 predictors should be considered for each split of the tree (see Appendix B). From here, we apply the model to our validation (testing) data set.  
```{r}
prediction <- predict(modelFit, filteredTest)
confusionMatrix(prediction, filteredTest$classe)
modelFit$finalModel
```  

Thus, the estimated accuracy of the model is 99.25%. The out-of-sample error rate is 0.83%.  
  
pmlTest <- pmlTest[,-53]
answers <- predict(modelFit, pmlTest)
answers
answers = c("B","A","B","A","A","E","D","B","A","A","B","C","B","A","E","E","A","B","B","B")

pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
### References  
James, G., Witten, D., Hastie, T., Tibshirani, R. (2013). *An Introduction to Statistical Learning: with Applications in R.* Springer, New York.
  
### Appendix A  
#### Plot of the correlation matrix  
```{r}
corrDF <- as.data.frame(lapply(filteredTrain, as.numeric))
correlation <- cor(corrDF)
col1 <- brewer.pal(10, "Spectral")
corrplot(correlation, method = "color", col = col1, 
         tl.cex = 0.3,
         tl.col = "black", tl.srt = 45)
```  
  
### Appendix B  
#### Plot of variable importance  
```{r}
modelImp <- varImp(modelFit)
plot(modelImp)
```
